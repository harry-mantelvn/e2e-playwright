# 🤖 AI-Powered Test Analysis Engine

This directory contains the AI-powered test analysis system that provides intelligent insights, failure categorization, and actionable recommendations for E2E test results.

## 📁 Directory Structure

```
ai-analysis/
├── analyze.py              # Main AI analysis script
├── requirements.txt        # Python dependencies
├── README.md              # This file
└── (output will be in ../automationexercise-e2e-pom/test-summary/)
```

## 🎯 What It Does

The AI analysis engine processes test results and provides:

1. **Failure Categorization** - AI-powered classification of test failures
   - Categories: FLAKY, INFRASTRUCTURE, CODE_BUG, TEST_BUG, ENVIRONMENT
   
2. **Root Cause Analysis** - Identifies underlying issues causing failures
   - Groups similar failures
   - Provides specific fix suggestions
   
3. **Flaky Test Detection** - Statistical analysis to identify unreliable tests
   - Calculates flakiness scores
   - Recommends quarantine or monitoring
   
4. **Performance Anomaly Detection** - Finds tests running slower than baseline
   - Uses ML-based anomaly detection
   - Identifies performance regressions
   
5. **Smart Recommendations** - Actionable steps prioritized by impact
   - Fix priorities
   - Investigation steps
   - Infrastructure suggestions

## 🚀 Quick Start

### Installation

```bash
pip install -r requirements.txt
```

### Usage

```bash
# Set OpenAI API key
export OPENAI_API_KEY="sk-proj-your-key-here"

# Run analysis (after tests have been executed)
python analyze.py
```

### Output

Analysis results are saved to:
```
../automationexercise-e2e-pom/test-summary/ai-analysis.json
```

## 📊 Input Data

The script expects test metrics at:
```
../automationexercise-e2e-pom/test-summary/metrics.json
```

Generated by: `npm run metrics:generate`

## 🔧 Configuration

### Environment Variables

| Variable | Required | Default | Description |
|----------|----------|---------|-------------|
| `OPENAI_API_KEY` | Yes* | - | OpenAI API key for GPT-4 access |

*If not provided, analysis runs in statistical-only mode (no AI).

### AI Model Settings

Default configuration (can be customized in `analyze.py`):
- **Model**: `gpt-4-turbo-preview`
- **Temperature**: `0.3` (lower = more consistent)
- **Max Tokens**: `300-400` per request

### Analysis Options

Control what gets analyzed:
```python
# In analyze.py
max_failures_to_analyze = 10  # Limit to control costs
enable_flaky_detection = True
enable_performance_analysis = True
```

## 📈 Output Format

### Summary Section
```json
{
  "summary": {
    "overall_health_score": 85,
    "trend": "STABLE",
    "total_failures_analyzed": 5,
    "flaky_tests_detected": 2,
    "performance_anomalies": 1,
    "ai_enabled": true
  }
}
```

### Failure Categorization
```json
{
  "failure_categorization": [
    {
      "test_name": "test-login",
      "category": "INFRASTRUCTURE",
      "confidence": 85,
      "reasoning": "Timeout errors suggest network issues",
      "suggested_action": "Check network stability and increase timeout"
    }
  ]
}
```

### Flaky Tests
```json
{
  "flaky_tests": [
    {
      "test_name": "test-search",
      "flakiness_score": 0.68,
      "pass_rate": 65.0,
      "recommendation": "QUARANTINE",
      "stability_trend": "DEGRADING"
    }
  ]
}
```

### Recommendations
```json
{
  "recommendations": [
    {
      "type": "ACTION",
      "priority": 1,
      "title": "Fix selector issues",
      "description": "2 tests failing due to outdated selectors",
      "impact": "HIGH",
      "actionable_steps": [
        "Update selectors in cart.page.ts",
        "Verify changes on staging"
      ]
    }
  ]
}
```

## 🔍 How It Works

### 1. Data Collection
```python
# Load test results
test_results = load_test_results('metrics.json')
historical_data = load_historical_data('historical-data.json')
```

### 2. AI Analysis (if enabled)
```python
# Use GPT-4 for semantic analysis
response = openai.chat.completions.create(
    model="gpt-4",
    messages=[...],
    temperature=0.3
)
```

### 3. Statistical Analysis
```python
# ML-based anomaly detection
from sklearn.ensemble import IsolationForest
clf = IsolationForest()
anomalies = clf.fit_predict(durations)
```

### 4. Report Generation
```python
# Generate comprehensive analysis
results = {
    "failure_categorization": [...],
    "flaky_tests": [...],
    "performance_anomalies": [...],
    "recommendations": [...]
}
```

## 💰 Cost Considerations

### OpenAI API Costs

**GPT-4 Turbo Pricing** (as of 2024):
- Input: ~$0.01 per 1K tokens
- Output: ~$0.03 per 1K tokens

**Typical Usage**:
- Per test run: ~10K tokens
- Cost per run: ~$0.10 USD
- Monthly (30 runs): ~$3.00 USD

### Cost Optimization Tips

1. **Limit failures analyzed**:
   ```python
   for failure in failed_tests[:5]:  # Analyze only first 5
   ```

2. **Use GPT-3.5 Turbo** (cheaper but less accurate):
   ```python
   model="gpt-3.5-turbo"  # ~10x cheaper
   ```

3. **Conditional analysis** (only on failures):
   ```bash
   if [ $FAILED_COUNT -gt 0 ]; then
     python analyze.py
   fi
   ```

4. **Statistical mode only** (free):
   ```bash
   # Don't set OPENAI_API_KEY
   python analyze.py  # Runs without AI
   ```

## 🐛 Troubleshooting

### "OpenAI API key not found"

**Solution**: Export the API key
```bash
export OPENAI_API_KEY="sk-proj-your-key"
```

### "Module 'openai' not found"

**Solution**: Install dependencies
```bash
pip install -r requirements.txt
```

### "Rate limit exceeded"

**Solution**: 
1. Check OpenAI usage dashboard
2. Reduce number of API calls
3. Wait for rate limit reset

### "No test results found"

**Solution**: Generate test metrics first
```bash
cd ../automationexercise-e2e-pom
npm run test:smoke:ci
npm run metrics:generate
```

## 📚 Dependencies

### Required Packages

```
openai>=1.3.0           # OpenAI API client
scikit-learn>=1.3.0     # ML algorithms
scipy>=1.11.0           # Statistical analysis
numpy>=1.24.0           # Numerical computations
```

### Python Version

Requires: **Python 3.11+**

Check version:
```bash
python --version
```

## 🔐 Security

### API Key Safety

- ✅ **DO**: Use environment variables
- ✅ **DO**: Store in GitHub Secrets for CI/CD
- ❌ **DON'T**: Hardcode in scripts
- ❌ **DON'T**: Commit to repository

### Best Practices

```bash
# Good: Environment variable
export OPENAI_API_KEY="sk-..."
python analyze.py

# Bad: Hardcoded
api_key = "sk-..."  # ❌ NEVER DO THIS
```

## 🧪 Testing

### Unit Tests (Coming Soon)

```bash
pytest test_analyze.py
```

### Manual Testing

```bash
# 1. Generate sample data
cd ../automationexercise-e2e-pom
npm run test:smoke:ci
npm run metrics:generate

# 2. Run analysis
cd ../ai-analysis
export OPENAI_API_KEY="your-key"
python analyze.py

# 3. Verify output
cat ../automationexercise-e2e-pom/test-summary/ai-analysis.json | jq '.'
```

## 📖 Documentation

- **Setup Guide**: `../AI-SETUP-GUIDE.md`
- **Quick Start**: `../AI-QUICK-START.md`
- **Full Proposal**: `../AI-ANALYSIS-PROPOSAL.md`

## 🚀 CI/CD Integration

This script is automatically run in GitHub Actions after test execution.

**Workflow file**: `../.github/workflows/e2e-automation.yml`

**AI Analysis job**:
```yaml
ai-analysis:
  runs-on: ubuntu-latest
  needs: test
  if: always()
  steps:
    - name: Run AI Analysis
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      run: python analyze.py
```

## 🎯 Roadmap

### Current Features (v1.0)
- ✅ AI-powered failure categorization
- ✅ Root cause analysis
- ✅ Flaky test detection
- ✅ Performance anomaly detection
- ✅ Smart recommendations

### Planned Features (v2.0)
- 🔄 Predictive failure analysis
- 🔄 Automatic test healing
- 🔄 Cross-repo pattern detection
- 🔄 Jira integration
- 🔄 Real-time analysis dashboard

## 🤝 Contributing

To add new analyzers or improve AI prompts:

1. Fork the repository
2. Create feature branch
3. Add your analyzer in `analyze.py`
4. Test thoroughly
5. Submit pull request

## 📝 License

Same as parent project.

## 💬 Support

- **Issues**: Create GitHub issue
- **Questions**: Check `AI-SETUP-GUIDE.md`
- **Discussions**: Team Slack channel

---

**Version**: 1.0.0  
**Last Updated**: January 2024  
**Maintained by**: QA Automation Team
